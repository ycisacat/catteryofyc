__author__ = 'yc'
#-*-coding:utf-8-*-
import re,chardet,urllib

class CrawlNews: #总是使用首字母大写单词串
    def getUrl(self):
        url="http://roll.news.sina.com.cn/interface/rollnews_ch_out_interface.php?col=89&spec=&type=&" \
            "ch=01&k=&offset_page=0&offset_num=0&num=5000&asc=&page=3&r=0.4589775952558407"
        text=urllib.urlopen(url).read()
        c=chardet.detect(text) #查询网站的编码方式
        code=c['encoding'] #直接print code:{'confidence': 0.99, 'encoding': 'GB2312'}
        # print code  #只打印encoding,字典
        text=str(text).decode(code,'ignore').encode('utf-8') #ignore忽略异常编码
        # print "text",text #5000条网页连接
   #抓取本页连接
        #link=r'<li>.*?</a>'
        linkPat=re.compile('http.*?shtml')
        l=re.findall(linkPat,text)
        return l #链接们

#获取新闻标题作为txt标题#
    def getTitle(self,psg):
        title=r'<title>(.*)</title>'
        titlePat=re.compile(title)
        mytitle=re.search(titlePat,psg)
        if mytitle:
            return mytitle.group(1).strip()
        else:
            return None

#写txt标题#
    def setTitle(self,mytitle):
        self.defaultTitle ="News"
        if mytitle is not None:
            mytitle=str(mytitle)
            print mytitle
            xiegang=re.compile('/')
            mytitle=re.sub(xiegang,'', mytitle)
            self.file=open(mytitle +'.txt','w+')
        else:
            self.file=open(self.defaultTitle +'.txt','w+')
        return mytitle

#去除正文多余内容
    def remove(self,x):
        removeA=re.compile('<a.*?</a>')
        removeExtra=re.compile('<.*?>|</.*?>')
        removeCPR=re.compile('Cop.*?Reserved')
        x=re.sub(removeA," ",x)
        x=re.sub(removeExtra," ",x)
        x=re.sub(removeCPR," ",x)
        x=x.replace("&nbsp;"," ").replace("┊"," ").replace("新浪公司"," ").replace("版权所有"," ")
        x=x.replace("电话：400-690-0000 欢迎批评指正"," ")
        return x.strip()

#获取新闻内容并写入txt
    def getData(self,psg):
        data=r'<p>(.*?)</p>'
        psgPat=re.compile(data,re.S) #re.S默认多行匹配
        mydata=re.findall(psgPat,psg) #文字,理论上时未经去除标签的正文
#        print "mydata",mydata #print结果为同contents
        contents=[]  #这里开始去除工作
        for a in mydata:
            content=self.remove(a)+"\n"
            contents.append(content)
#            print "contents",contents #print的结果为 ['\xe3\x80\x80\xe3\x80\x80\xe6\x96\xb0\xe6\xb5\']
        for j in contents:
            self.file.write(j)
        self.file.close()
        return psg

spider = crawlNews()
l=spider.getUrl()
print "总共有",len(l),"篇新闻",
for i in l:   #解码
    num=l.index(i)
    print "正在输出第",num+1,"篇新闻"
    web=urllib.urlopen(i).read()
    code2=chardet.detect(web) #查询网站的编码方式
    cw=code2['encoding'] #直接print code:{'confidence': 0.99, 'encoding': 'GB2312'}
    web=str(web).decode(cw,'ignore').encode('utf-8')
    pageCode = web
    title = spider.getTitle(pageCode)
    txtTitle=spider.setTitle(title)
    passage=spider.getData(pageCode)

